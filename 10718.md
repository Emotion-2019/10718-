# 과제

## 수치예측

 ### 선형 회귀

   - 독립 변수의 일차식으로 된 회귀이다.
   - 통계학에서, 선형 회귀는 종속 변수 y와 한 개 이상의 독립 변수 X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다
   - 딥러닝의 기본이 된다.
   - 예) 일한 시간에 따라 만드는 물건의 개수가 다르다.

  ## 손실함수와 경사하강법

 ### 손실함수

   - 신경망이 학습할 수 있도록 해주는 지표
   - 머신러닝 모델의 출력값과 아용자가 원하는 출력값 차이 , 오차를 말함

  - 이 손실 함수 값이 최소화 되도록 하는 가중치와 편향을 찾는 것이 학습

    ​	

### 경사하강법

 - 1차 근삿값 발견용 최적화 알고리즘

 - 함수의 기울기를 구하여 기울기가 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복

 - 산을 오른다고 생각하면 경사가 높아지는 쪽으로 계속 움직여 정상에 도착 할 것이고 내려간다면 경사가 낮아지는 쪽으로 움직일 것인데 이러한 원리로 이해하면 된다.

 - 학습률은 기본적으로 높아 빠르게 학습 되지만 성능이 좋지 않은 편이라 한다.

   

##  이진 분류

### 퍼셉트론

- 인공신경망의 한 종류이다
- 다수의 신호를 입력 받아 하나의 신호를 출력한다.
- 뉴런이 전기신호를 내보내 정보를 전달하는 것과 비슷
- 노드의 가중치와 입혁치를 곱한 것을 모두 합한 값이 임계치보다 크면 활성화 되며 1을 출력
  - 활성화 되지 않으면 결과값으로 0을 출력

### 로지스틱 회귀 

- 범주형 변수를 예측하는 모델이다.
- 두 카테고리 중 하나로 분류 하려 생김
- 성장곡선을 표현하는 수리적 모형 등으로 활용됨

### 시그모이드 함수

-  S자형 곡선 또는 시그모이드 곡선을 가지는 수학 함수이다.

-  앞의 로지스틱 함수도 그중 하나

-   시그모이드 함수는 실함수로써 유계이며 미분가능한 함수이며, 모든 점에서의 미분값은 양수이다.

- 단조 함수이며 1차 미분 그래프 

  ![image-20191106094743775](C:\Users\lg\AppData\Roaming\Typora\typora-user-images\image-20191106094743775.png)

  

 

